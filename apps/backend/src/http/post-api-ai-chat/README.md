# AI Chat Backend Proxy - Streaming Implementation

This endpoint provides a secure server-side streaming proxy for AI chat functionality following the [Building Real-Time AI Streaming Services](https://metaduck.com/building-real-time-ai-streaming-services/) architecture.

## Architecture

- Uses Lambda Function URLs with `InvokeMode: RESPONSE_STREAM`
- Real-time token-by-token streaming using `lambda-stream`
- AI SDK `pipeDataStreamToResponse` for seamless streaming
- API key stored server-side only (environment variable)

## Endpoint

- `POST /api/ai/chat` - Handles streaming AI chat requests via Lambda URL

## Request Format

```json
{
  "messages": [
    {
      "role": "user",
      "content": "Hello, how can I help you?"
    }
  ]
}
```

## Response Format

The response is streamed as `text/event-stream` with real-time tokens:

```
0:"Hello"
0:" and"
0:" welcome"
0:"!"
```

Tokens arrive incrementally as they're generated by the AI model.

## Key Implementation Details

### Backend Handler

```typescript
export const handler = streamifyResponse(
  async (event, responseStream: ResponseStream) => {
    // Configure streaming response
    responseStream = ResponseStream.from(responseStream, {
      statusCode: 200,
      headers: {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
      },
    });

    // Pipe AI stream directly to Lambda response
    const result = streamText({ model, messages });
    await pipeDataStreamToResponse(result, responseStream);
  }
);
```

### Frontend Streaming Client

```typescript
const reader = response.body?.getReader();
const decoder = new TextDecoder();
let fullText = "";

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  const chunk = decoder.decode(value, { stream: true });
  fullText += chunk;

  // Update UI in real-time
  await saveNewMessage({ ... });
}
```

## Deployment

This endpoint uses the `@lambda-urls` directive in `app.arc`, which:

- Creates Lambda Function URL with streaming enabled
- Configures CORS for browser access
- Supports local development via Architect sandbox

## Local Development

Architect sandbox automatically handles streaming in local environment:

```bash
pnpm dev
```

## Security

- API key is stored server-side only (environment variable)
- No client has access to the Gemini API key
- Uses Vercel AI SDK for secure API communication
- CORS configured for frontend integration

## Environment Variables

- `GEMINI_API_KEY`: Google Gemini API key (required)
